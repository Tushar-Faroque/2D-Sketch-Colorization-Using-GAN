{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CSE 465 Project"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install -U tensorflow==2.2.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential, load_model\n\nimport os\nimport time\n\nfrom matplotlib import pyplot as plt\n\n# Change PATH variable to absolute/ relative path to the images directory on your machine which contains the train and val folders\nPATH = '../input/anime-sketch-colorization-pair/data'\n\n# Change these variables as per your need\nEPOCHS = 100\n# BUFFER_SIZE is used when we shuffle the data samples while training. \n# Higher the value of this more will be the degree of shuffling, and hence, \n# higher will be the accuracy of the model.But with large data, it takes a lot of processing power to shuffle the images. \nBUFFER_SIZE = 1000 #14224\nBATCH_SIZE = 10\n# originally it is 512, for faster training we have chosen 256X256\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"def load(image_file):\n    # read the image file and make it a tensor string\n    image = tf.io.read_file(image_file)\n    # Decode a PNG-encoded image to a uint8(by default)\n    image = tf.image.decode_png(image)\n    \n    w = tf.shape(image)[1]\n    print(f'w:{w}\\n')\n\n    w = w // 2\n    print(f'w//2:{w}\\n')\n    \n    # seperate the real image and the input image from the 512X1024(height*width) image\n    real_image = image[:, :w, :] # height, width, channel\n    print(f'real_image = {real_image}\\n')\n    print(f'real_image = {tf.shape(real_image)}\\n')\n    input_image = image[:, w:, :] # height, width, channel\n    print(f'input_image = {input_image}\\n')\n    print(f'input_image = {tf.shape(input_image)}\\n')\n\n    # now make/cast those tensors in float32 type\n    input_image = tf.cast(input_image, tf.float32)\n    print(f'input_image:{input_image}\\n')\n    real_image = tf.cast(real_image, tf.float32)\n    print(f'real_image:{real_image}\\n')\n\n    return input_image, real_image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"As you can see in the images below\nthat they are going through random jittering,\nRandom jittering as described in the paper is to\n\n1. Resize an image to bigger height and width\n2. Randomly crop to the target size\n3. Randomly flip the image horizontally"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp, re = load(PATH+'/train/1005024.png')\n# casting to int for matplotlib to show the image\nplt.figure()\nplt.imshow(inp/255.0)\nplt.figure()\nplt.imshow(re/255.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def resize(input_image, real_image, height, width):\n    input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    print(f'resize --> input_image:{tf.shape(input_image)}\\n')\n    real_image = tf.image.resize(real_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    print(f'resize --> real_image:{tf.shape(real_image)}\\n')\n\n    return input_image, real_image\n\ndef random_crop(input_image, real_image):\n    stacked_image = tf.stack([input_image, real_image], axis=0)\n    print(f'random_crop --> stacked_image:{tf.shape(stacked_image)}\\n')\n    cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n    print(f'random_crop --> cropped_image:{tf.shape(cropped_image)}\\n')\n\n    return cropped_image[0], cropped_image[1]\n\n# normalizing the images to [-1, 1]\ndef normalize(input_image, real_image):\n    input_image = (input_image / 127.5) - 1\n    print(f'normalize --> input_image:{tf.shape(input_image)}\\n')\n    real_image = (real_image / 127.5) - 1\n    print(f'normalize --> real_image:{tf.shape(real_image)}\\n')\n\n    return input_image, real_image\n\n@tf.function() # @tf.function in order to turn plain Python code into graph.\ndef random_jitter(input_image, real_image):\n    # resizing to 286 x 286 x 3\n    input_image, real_image = resize(input_image, real_image, 286, 286)\n    # randomly cropping to 256 x 256 x 3\n    input_image, real_image = random_crop(input_image, real_image)\n\n    # random mirroring\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        real_image = tf.image.flip_left_right(real_image)\n\n    return input_image, real_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for testing the settings\nplt.figure(figsize=(8, 8))\nfor i in range(4):\n    rj_inp, rj_re = random_jitter(inp, re)\n    plt.subplot(2, 2, i+1)\n    plt.imshow(rj_inp/255.0)\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Train & Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image_train(image_file):\n    input_image, real_image = load(image_file)\n    input_image, real_image = random_jitter(input_image, real_image)\n    input_image, real_image = normalize(input_image, real_image)\n\n    return input_image, real_image\n    \ntrain_dataset = tf.data.Dataset.list_files(PATH+'/train/*.png')\ntrain_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE) # automatically tune performance knobs\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image_test(image_file):\n    input_image, real_image = load(image_file)\n    input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH)\n    input_image, real_image = normalize(input_image, real_image)\n\n    return input_image, real_image\n  \ntest_dataset = tf.data.Dataset.list_files(PATH+'/val/*.png')\ntest_dataset = test_dataset.map(load_image_test)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Generator Model"},{"metadata":{},"cell_type":"markdown","source":"## Build the Generator\n  * The architecture of generator is a modified U-Net.\n  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n  * There are skip connections between the encoder and decoder (as in U-Net).\n  * U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany."},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT_CHANNELS = 3 # because of the RGB channel\n\n# used for compressing the image\ndef downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n\n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n        \n    # LeakyReLU --> allowing some negative values to pass through.The range of the Leaky ReLU is (-infinity to infinity) \n    # The whole idea behind making the Generator work is to receive gradient values from the Discriminator, \n    # and if the network is stuck in a dying state situation, the learning process won’t happen.\n    result.add(tf.keras.layers.LeakyReLU())\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"down_model = downsample(3, 4)\ndown_result = down_model(tf.expand_dims(inp, 0))\nprint (down_result.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to make the image from to its original size\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    result = tf.keras.Sequential()\n    result.add(\n    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n\n    result.add(tf.keras.layers.BatchNormalization())\n\n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n    \n    # ReLU --> take the maximum between the input value and zero.\n    # If we use the ReLU activation function, \n    # sometimes the network gets stuck in a popular state called the dying state, \n    # and that’s because the network produces nothing but zeros for all the outputs.\n    result.add(tf.keras.layers.ReLU())\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"up_model = upsample(3, 4)\nup_result = up_model(down_result)\nprint (up_result.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def buildGenerator():\n    inputs = tf.keras.layers.Input(shape=[256,256,3])\n\n    # bs -> Batch Size\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    # The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s-shaped).\n    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                           strides=2,\n                                           padding='same',\n                                           kernel_initializer=initializer,\n                                           activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    print(f'skips = {skips}\\n')\n\n    skips = reversed(skips[:-1])\n    print(f'skips2 = {skips}\\n')\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)\n\ngenerator = buildGenerator()\ngenerator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = buildGenerator()\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Discriminator Model"},{"metadata":{},"cell_type":"markdown","source":"## Build the Discriminator\n  * The Discriminator is a PatchGAN.\n  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n  * Discriminator receives 2 inputs.\n    * Input image and the target image, which it should classify as real.\n    * Input image and the generated image (output of generator), which it should classify as fake.\n    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)"},{"metadata":{},"cell_type":"markdown","source":"* PatchGAN is a type of discriminator for generative adversarial networks which only penalizes structure at the scale of local image patches. The PatchGAN discriminator tries to classify if each  patch in an image is real or fake. This discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of . Such a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. It can be understood as a type of texture/style loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"def buildDiscriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n\n    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n\n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n  \ndiscriminator = buildDiscriminator()\ndiscriminator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator = buildDiscriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Functions for the Models"},{"metadata":{},"cell_type":"markdown","source":"* **Generator loss**\n  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n  * This allows the generated image to become structurally similar to the target image.\n  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\nLAMBDA = 100\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss, gan_loss, l1_loss\n\n# print('::::generator_loss::::\\n')\n# print(f'total_gen_loss:{total_gen_loss}\\n, gan_loss:{gan_loss}\\n, l1_loss:{l1_loss}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Discriminator loss**\n  * The discriminator loss function takes 2 inputs; **real images, generated images**\n  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n  * Then the total_loss is the sum of real_loss and the generated_loss\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss\n# print('::::discriminator_loss::::\\n')\n# print(f'total_disc_loss:{total_disc_loss}\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# beta_1 is the exponential decay rate, by default it is 0.9\ngenerator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Checkpoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './Sketch2Color_training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying Output Images"},{"metadata":{},"cell_type":"markdown","source":"## Generate Images\n\nWrite a function to plot some images during training.\n\n* We pass images from the test dataset to the generator.\n* The generator will then translate the input image into the output.\n* Last step is to plot the predictions and **voila!**"},{"metadata":{},"cell_type":"markdown","source":"Note: The `training=True` is intentional here since\nwe want the batch statistics while running the model\non the test dataset. If we use training=False, we will get\nthe accumulated statistics learned from the training dataset\n(which we don't want)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_images(model, test_input, tar):\n    print(\"Displaying Output Images\")\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15,15))\n\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logging the Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nlog_dir=\"Sketch2Coloe_logs/\"\n\nsummary_writer = tf.summary.create_file_writer(\n  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Step"},{"metadata":{},"cell_type":"markdown","source":"## Training\n\n* For each example input generate an output.\n* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n* Next, we calculate the generator and the discriminator loss.\n* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n* Then log the losses to TensorBoard."},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(input_image, target, epoch):\n    # gen_tape-> compute gradients for generator\n    # disc_tape-> compute gradients for discriminator\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n        \n        # giving the discriminator sketch image and target image\n        disc_real_output = discriminator([input_image, target], training=True)\n        # giving the discriminator sketch image and the colorized image by generator\n        disc_generated_output = discriminator([input_image, gen_output], training=True)\n        \n        \n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n\n    with summary_writer.as_default():\n        tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n        tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define the fit() function"},{"metadata":{},"cell_type":"markdown","source":"The actual training loop:\n\n* Iterates over the number of epochs.\n* On each epoch it clears the display, and runs `generate_images` to show it's progress.\n* On each epoch it iterates over the training dataset, printing a '.' for each example.\n* It saves a checkpoint every 20 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(train_ds, epochs, test_ds):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for example_input, example_target in test_ds.take(1):\n            generate_images(generator, example_input, example_target)\n        print(\"Epoch: \", epoch)\n        \n        # Train\n        for n, (input_image, target) in train_ds.enumerate():\n            print('.', end=\"\")\n            if (n+1) % 100 == 0:\n                print()\n            train_step(input_image, target, epoch)\n        print()\n\n        # saving (checkpoint) the model every 20 epochs\n        if (epoch + 1) % 20 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))\n    checkpoint.save(file_prefix = checkpoint_prefix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorborad Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir {log_dir}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now start the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(train_dataset, EPOCHS, test_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Restore Latest Checkpoint or Load the saved H5 file"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\n# model = keras.models.load_model('../input/h5-model/2d_Skatch_Colorization_Model.h5')\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n# print(checkpoint)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Outputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"for example_input, example_target in test_dataset.take(5):\n    generate_images(generator, example_input, example_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator.save('2d_Skatch_Colorization_Model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate using the h5 file for your skatched Images"},{"metadata":{},"cell_type":"markdown","source":"### Load the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nmodel = keras.models.load_model('../input/h5-model/2d_Skatch_Colorization_Model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pre-process the images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image)\n\n    w = tf.shape(image)[1]\n    print(f'w:{w}')\n\n    w = w // 2\n    print(f'w//2:{w}')\n    \n    input_image = image[:, :, :]\n    print(f'input_image:{input_image}')\n\n    input_image = tf.cast(input_image, tf.float32)\n    print(f'input_image:{input_image}')\n    \n    return input_image\n\ndef resize(input_image, height, width):\n    input_image = tf.image.resize(input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    return input_image\n\n\ndef normalize(input_image):\n    input_image = (input_image / 127.5) - 1\n\n    return input_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image_test2(image_file):\n    input_image = load(image_file)\n    input_image = resize(input_image, IMG_HEIGHT, IMG_WIDTH)\n    input_image = normalize(input_image)\n\n    return input_image\n  \ntest_dataset = tf.data.Dataset.list_files('../input/skatch-images'+'/*.jpg')\ntest_dataset = test_dataset.map(load_image_test2)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_images2(model, test_input):\n    print(\"Displaying Output Images\")\n    prediction = model(test_input, training=True)\n#     print(f'test_input: {test_input}')\n#     print(f'prediction: {prediction}')\n    plt.figure(figsize=(15,15))\n\n    display_list = [test_input[0], prediction[0]]\n    title = ['Input Image', 'Predicted Image']\n\n    for i in range(2):\n        plt.subplot(1, 2, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    for example_input in test_dataset.take(10):\n        generate_images2(model, example_input)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}